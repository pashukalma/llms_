{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q 'google-generativeai>=0.8.3'"
      ],
      "metadata": {
        "id": "NmCcuPItLkyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjxSCCnjIujR",
        "outputId": "60e39f61-1f6f-433b-84c5-7eb7f287894d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cgi, math, os, pickle, random, re, socket, sys, time, urllib\n",
        "import datetime, re, sys, time\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "'''\n",
        "!curl \"https://generativelanguage.googleapis.com/v1bets/models/gemeni-1.5-flash:generateContent?key=!GOOGLE_API_KEY\" \\\n",
        "-H 'Content-Type: application/json' -X POST \\\n",
        "- d '{\"contents:[{\"parts\":[{\"text\": \"Please give me the Python code to generate heapsort\"}]}]}'\n",
        "'''"
      ],
      "metadata": {
        "id": "mG9ma-32IwyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Selecting models"
      ],
      "metadata": {
        "id": "s6XLBQhGGivt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "nRTtlKl1Ayr4",
        "outputId": "fbb0c7c1-b523-4996-e4f6-5f2af6fd7176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-exp-1206\n",
            "models/gemini-exp-1121\n",
            "models/gemini-exp-1114\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/aqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if model.name == 'models/gemini-2.0-flash-thinking-exp-01-21':\n",
        "    print(model); break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "vdbQzXQ-GfZw",
        "outputId": "7ac00cf1-4a52-4d60-d3d2-d6016440ef6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "high_temperature_model = genai.GenerativeModel('gemini-1.5-flash',\n",
        "          generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "\n",
        "retry_policy = {\n",
        "    \"retry\": retry.Retry(predicate=retry.if_transient_error,\n",
        "                         initial=10, multiplier=1.5, timeout=300) }\n",
        "for _ in range(5):\n",
        "  response = high_temperature_model.generate_content(\n",
        "      'Pick a random color', request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '_'*20)"
      ],
      "metadata": {
        "id": "Cyz2P1xEIZ2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top-K and top-P"
      ],
      "metadata": {
        "id": "RWgMNOOANHmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95)\n",
        ")\n",
        "story_prompt = 'query - goes here '\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "HGA2_ujlKtSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompting\n",
        "- zero shot, enum mode, one-shot and few-shot - json\n",
        "- chain-of-thought (COT)\n",
        "- reason and act (ReAct)"
      ],
      "metadata": {
        "id": "VqTOaUtFQ9mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Zero shot '''\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5 )\n",
        ")\n",
        "zero_shot_prompt = '''\n",
        "Classify movie reviews as positive, neutral, negative\n",
        "Review: I wish there were more movies like this masterpiece\n",
        "Sentiment:\n",
        "'''\n",
        "response = model.generate_content(\n",
        "    zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "36l2gpf1Q_eR",
        "outputId": "914fe531-8ac1-4adf-a89a-74be571e0672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: **Positive**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' enum mode '''\n",
        "import enum\n",
        "class Sentiment(enum.Enum):\n",
        "  positive = 'positive'\n",
        "  neutral = 'neutral'\n",
        "  negative = 'negative'\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        response_mime_type = 'text/x.enum',\n",
        "        response_schema=Sentiment)\n",
        ")\n",
        "\n",
        "response = model.generate_content(\n",
        "    zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yC151tphRf3H",
        "outputId": "37ca7f4d-aa03-40b9-bb25-4ef8340d3898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' one-shot and few-shot '''\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250 )\n",
        ")\n",
        "few_shot_prompt = '''\n",
        "parse a customer's order into valid json\n",
        "\n",
        "EXAMPLE:\n",
        "small order with item-1, item-2, and item-3\n",
        "Json response:\n",
        "{ \"size\": \"small\", \"type\": normal, \"items\": [\"item-1\", \"item-2\", \"item-3\"] }\n",
        "\n",
        "EXAMPLE:\n",
        "large order with item-2, item-4, and item-5\n",
        "Json response:\n",
        "{ \"size\": \"small\", \"type\": normal, \"items\": [\"item-1\", \"item-2\", \"item-3\"] }\n",
        "Json response:\n",
        "{ \"size\": \"large\", \"type\": normal, \"items\": [\"item-2\", \"item-4\", \"item-5\"] }\n",
        "Order:\n",
        "'''\n",
        "\n",
        "customer_order = \"Give me large order with item-1, item-6\"\n",
        "response = model.generate_content([few_shot_prompt, customer_order],\n",
        "              request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "ESerTHW6Rf-Y",
        "outputId": "8d35f1c8-5f07-4c3f-e516-2acaca78f07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"type\": \"normal\",\n",
            "  \"items\": [\"item-1\", \"item-6\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' json mode '''\n",
        "import typing_extensions as typing\n",
        "\n",
        "class Order(typing.TypedDict):\n",
        "  size: str\n",
        "  ingredients: list[str]\n",
        "  type: str\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Order)\n",
        ")\n",
        "response = model.generate_content(\n",
        "    'Can I have a large order with item-1 and item-2')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mRyROcKQRgCm",
        "outputId": "eff11785-40f7-4d9c-c3c5-eef35a6df38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ingredients\": [\"item-1\", \"item-2\"], \"size\": \"large\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Chain of thought '''\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "prompt = ''' ccontent here  '''\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "c6md7HSFRgF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' ReAct - reason and act '''\n",
        "\n",
        "model_instructions = '''\n",
        "\n",
        "Solve a question answering with interleaving Thought, Action, Observation steps.\n",
        "Thought can reason about current situation.\n",
        "Observation is understanding relevant information from Action's output\n",
        "Action can be one of the three types:\n",
        "(1) <search>entity</search>, searches exact entity in wikipedia and return the\n",
        "first paragraph if it exists. If not, it will return similar entities to search\n",
        "and you can try information from those topics\n",
        "(2) <lookup>keyword</lookup> return next sentence containing keyword in the\n",
        "current context. This only does exact matches\n",
        "(3) <finish>answer</finish> returns the answer and finishes the task.\n",
        "'''\n",
        "\n",
        "response = react_chat.send_message(observation, generation_config=config,\n",
        "            request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "xgr-McLXHYG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating code"
      ],
      "metadata": {
        "id": "kEqg2TpEQ12-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024)\n",
        ")\n",
        "code_prompt = '''\n",
        "write a Python function to calculate the factorial of a number.\n",
        "no explanation, provide code only\n",
        "'''\n",
        "retry_policy = {'retry': retry.Retry(predicate=retry.if_transient_error)}\n",
        "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ioFIfaxHGfhG",
        "outputId": "15988573-8fcc-4a92-b658-3ab4f1e553d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' automatically run the generated code, and return the response '''\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest', tools='code_execution',)\n",
        "code_prompt = '''\n",
        "calculate the sum of the first 14 prime numbers, only consider the odd primes,\n",
        "and make sure they are counted all\n",
        "'''\n",
        "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "5xGCfjSvSQwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print('*-------------*')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h34_gqZSQ5B",
        "outputId": "ff19e122-a1bb-4506-9405-9fcca67b85c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: \"To calculate the sum of the first 14 odd prime numbers, I will first generate a list of prime numbers, filter out the even number 2, and then sum the first 14 remaining numbers.\\n\\n\"\n",
            "\n",
            "*-------------*\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\nimport sympy\\n\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    return sympy.isprime(n)\\n\\nprimes = []\\nnum = 2\\ncount = 0\\nwhile count < 15: # generate at least 14 odd primes, since we will remove 2 later\\n    if is_prime(num):\\n        primes.append(num)\\n        count += 1\\n    num += 1\\n\\nodd_primes = [p for p in primes if p != 2]\\nsum_of_primes = sum(odd_primes[:14])  # Sum the first 14 odd primes\\n\\nprint(f\\'{primes=}\\')\\nprint(f\\'{odd_primes=}\\')\\nprint(f\\'{sum_of_primes=}\\')\\n\\n\"\n",
            "}\n",
            "\n",
            "*-------------*\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "  output: \"primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nodd_primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes=326\\n\"\n",
            "}\n",
            "\n",
            "*-------------*\n",
            "text: \"The sum of the first 14 odd prime numbers is 326.  The code first generates a list of prime numbers until it has at least 14 odd primes (since we will remove 2). Then, it filters out the even number 2 and sums the first 14 elements of the resulting list.\\n\"\n",
            "\n",
            "*-------------*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl \"https://github.com/google-gemini/cookbook/blob/main/quickstarts/Code_Execution.ipynb\"\n",
        "explain_prompt = f'''\n",
        "Explain what this file does, very high level\n",
        "...\n",
        "{file_contents}\n",
        "...\n",
        "'''\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "KvV93B_MSQ9V",
        "outputId": "a8ecaf3f-93ae-428c-9c87-3dcafe319f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a HTML representation of a GitHub page displaying a Jupyter Notebook file (`.ipynb`).  It includes the notebook's content rendered as HTML, along with the standard GitHub header, navigation, and footer.  The HTML heavily uses embedded JavaScript and React components to dynamically load and display the notebook content and interact with the GitHub interface.  Essentially, it's a web page designed to show a specific Jupyter Notebook file within the context of a GitHub repository.\n"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "explain_prompt = f'''\n",
        "Explain what this file does, very high level\n",
        "...\n",
        "{file_contents}\n",
        "...\n",
        "'''\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "PRive46xSRJe",
        "outputId": "d2b9d7bf-885a-4f1d-f132-e85255c82586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This Bash script (`bash-git-prompt`) enhances the command prompt to display Git repository information (branch, status, etc.).  It does this by:\n\n1. **Loading a Theme:** It selects a color theme for the prompt from a set of predefined themes or a custom user-defined theme.\n\n2. **Fetching Git Status:** It runs a Git command to obtain the repository's status (branch, changes, etc.).  It handles the case where the status command may not be available, and attempts to locate a suitable alternative.\n\n3. **Formatting the Prompt:** It formats the Git status information, incorporating the selected theme colors,  branch name, changes, and other configurable elements to create a visually informative prompt.  It also handles edge cases such as a detached HEAD state or no Git repository being present.\n\n4. **Integrating with the Shell:** It integrates its prompt function into the shell's `PROMPT_COMMAND` variable to ensure the prompt is updated automatically after each command execution.  It smartly handles various bash versions.  Additionally, it handles various virtual environments and displays their information in the prompt.\n\n5. **Providing Customization:**  It allows users to customize the prompt's appearance (colors, symbols, information displayed) via configuration files and environment variables.\n\nIn essence, it's a sophisticated prompt customization script tailored for Git users to provide real-time repository status directly in their shell.\n"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings and similarity scores"
      ],
      "metadata": {
        "id": "q-YQP1yY0w72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' cosine similarity '''\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "  vectorizer = TfidfVectorizer(\n",
        "      stop_words = 'english',\n",
        "      use_idf = True,\n",
        "      norm = 'l2',\n",
        "      ngram_range = (1, 2),\n",
        "      sublinear_tf = True,\n",
        "      analyzer = 'word'\n",
        "  )\n",
        "\n",
        "  tfidf = vectorizer.fit_transform([text1, text2])\n",
        "  similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "  return similarity[0][0]"
      ],
      "metadata": {
        "id": "nhHzhlMK3-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifying Embeddings with Keras and Gemini API"
      ],
      "metadata": {
        "id": "VsBi3kDvr6S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' create embeddings\n",
        "\n",
        "- retrieval query\n",
        "- retrieval document\n",
        "- semantic similarity\n",
        "- classification\n",
        "- clustering\n",
        "- fact_verification\n",
        "'''\n",
        "from google.api_core import retry\n",
        "from tqdm.rich import tqdm\n",
        "tqdm.pandas()\n",
        "@retry.Retry(timeout=300.0)\n",
        "def embed_fn(text: str) -> list[float]:\n",
        "  response = genai.embed_content(\n",
        "      model='models/text-embedding-004', content=text, task_type='classification')\n",
        "  return response['embedding']\n",
        "\n",
        "def create_embeddings(df):\n",
        "  df['Embeddings'] = df['Text'].progress_apply(embed_fn)\n",
        "  return df"
      ],
      "metadata": {
        "id": "9N0Odn8l02lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build a Classification Model"
      ],
      "metadata": {
        "id": "K63NFqro0lDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "def build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n",
        "  return keras.Sequential(\n",
        "      [ layers.Input([input_size], name='embedding_inputs'),\n",
        "        layers.Dense(input_size, activation='relu', name='hidden'),\n",
        "        layers.Dense(num_classes, activation='softmax', name='output_probs') ] )\n",
        "\n",
        "embedding_size = len(df_train['Embeddings'].iloc[0])\n",
        "classifier = build_classification_model(\n",
        "    embedding_size, len(df_train['Class Name'].unique()))\n",
        "\n",
        "classifier.summary()\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "E4Wmw5Sb4uHP",
        "outputId": "2fbc623a-d54c-4111-c30e-03b609c8f458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ hidden (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │         \u001b[38;5;34m590,592\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_probs (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m3,076\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_probs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' train the model '''\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "y_train = df_train['Encoded Label']\n",
        "x_train = np.stack(df_train['Embeddings'])\n",
        "y_val = df_test['Encoded Label']\n",
        "x_val = np.stack(df_test['Embeddings'])\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
        "history = classifier.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stop],\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs\n",
        ")\n",
        "''' evaluate model performance '''\n",
        "classifier.evaluate(x=x_val, y=y_val, return_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeemLJwf7rM3",
        "outputId": "dae96c1e-e9e7-4696-e723-b88a1627c771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0976 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9950000047683716, 'loss': 0.07911157608032227}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded = embed_fn(new_text)\n",
        "input_batched = np.array([embedded])\n",
        "[result] = classifier.predict(input_batched)\n",
        "for idx, category in enumerate(df_test['Class Name'].cat.categories):\n",
        "  print(f\" {category}: {result[idx]*100:0.2f}%\")"
      ],
      "metadata": {
        "id": "O6lvZnYt4ue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function Calling with Gemini"
      ],
      "metadata": {
        "id": "Ydwdacuw-17P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext sql\n",
        "%sql sqlite:///sample.db"
      ],
      "metadata": {
        "id": "3QtHnJ0f4ujN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE TABLE IF NOT EXISTS products(\n",
        "  product_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "  product_name VARCHAR(255) NOT NULL,\n",
        "  price DECIMAL(10, 2) NOT NULL ) ;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS staff(\n",
        "  staff_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "  first_name VARCHAR(255) NOT NULL,\n",
        "  last_name VARCHAR(255) NOT NULL ) ;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS orders(\n",
        "  order_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "  customer_name VARCHAR(255) NOT NULL,\n",
        "  staff_id INTEGER NOT NULL,\n",
        "  product_id INTEGER NOT NULL,\n",
        "  FOREIGN KEY (staff_id) REFERENCES staff (staff_id),\n",
        "  FOREIGN KEY (product_id) REFERENCES product (product_id) ) ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAGYLzyL4unl",
        "outputId": "f691398f-8516-453d-a2d4-e355b8acb71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * sqlite:///sample.db\n",
            "Done.\n",
            "Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "INSERT INTO products(product_name, price) VALUES\n",
        "('laptop', 1200.00), ('iPhone', 799.00), ('usb charger', 25.00) ;\n",
        "\n",
        "INSERT INTO staff(first_name, last_name) VALUES\n",
        "('firstname1', 'lastname1'), ('firstname2', 'lastname2'), ('firstname3', 'lastname3');\n",
        "\n",
        "INSERT INTO orders (customer_name, staff_id, product_id) VALUES\n",
        "('cust1', 1, 1), ('cust2', 2, 2), ('cust3', 1, 3) ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBTO3FJy4uyr",
        "outputId": "05d047e3-3652-4301-b0c9-843b42ca1316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * sqlite:///sample.db\n",
            "3 rows affected.\n",
            "3 rows affected.\n",
            "3 rows affected.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "db_file = 'sample.db'\n",
        "db_conn = sqlite3.connect(db_file)\n",
        "cursor = db_conn.cursor()\n",
        "cursor.execute(\"SELECT name from sqlite_master where type='table'; \")\n",
        "tables = cursor.fetchall()\n",
        "list_of_tables = [t[0] for t in tables]\n",
        "list_of_tables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoOKDnMz4u5O",
        "outputId": "ea457cb7-d4d7-43b1-85ee-477d71f2bcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['products', 'sqlite_sequence', 'staff', 'orders']"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(f'PRAGMA table_info(products);')\n",
        "schema = cursor.fetchall()\n",
        "[(col[1], col[2]) for col in schema]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3pYizhk03AG",
        "outputId": "5167045e-7533-4c2e-81f4-82264c9c959c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('product_id', 'INTEGER'),\n",
              " ('product_name', 'VARCHAR(255)'),\n",
              " ('price', 'DECIMAL(10, 2)')]"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute('select * from products')\n",
        "cursor.fetchall()"
      ],
      "metadata": {
        "id": "UUn0HnFL03EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' create db tools '''\n",
        "def list_tables() -> list[str]:\n",
        "  print('DB Call: list_tables')\n",
        "  cursor = db_conn.cursor()\n",
        "  cursor.execute(\"SELECT name from sqlite_master where type='table'; \")\n",
        "  tables = cursor.fetchall()\n",
        "  return [t[0] for t in tables]\n",
        "\n",
        "def describe_table(table_name: str) -> list[tuple[str,str]]:\n",
        "  print('DB Call: describe_tables')\n",
        "  cursor.execute(f'PRAGMA table_info({table_name});')\n",
        "  schema = cursor.fetchall()\n",
        "  return [(col[1], col[2]) for col in schema]\n",
        "\n",
        "def execute_query(sql: str) -> list[list[str]]:\n",
        "  print('DB Call: execute_query')\n",
        "  cursor.execute(sql)\n",
        "  return cursor.fetchall()\n",
        "\n",
        "list_tables(), describe_table('products'), execute_query('select * from products')"
      ],
      "metadata": {
        "id": "TpBL8R8k03Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_tools = [list_tables, describe_table, execute_query]\n",
        "\n",
        "instruction = '''\n",
        "You are a helpful chatbot that can interact with an SQL database for a computer\n",
        "store. You will take the users questions and turn them into SQL queries using\n",
        "the tools available. Once you have the information you need, you will answer the\n",
        "user's question using the data returned.\n",
        "Use list_tables to see what tables are present, describe_table to understand\n",
        "the schema, and execute_query to issue an SQL SELECT query.\n",
        "'''\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest',\n",
        "                              tools=db_tools, system_instruction=instruction)\n",
        "\n",
        "retry_policy = {'retry': retry.Retry(predicate=retry.if_transient_error)}\n",
        "\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "r_ = chat.send_message('what is the cheapest product', request_options=retry_policy)\n",
        "print(r_.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "KawOrZzl03pW",
        "outputId": "4d347651-3652-4945-b07b-2e5668c363a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DB Call: list_tables\n",
            "DB Call: describe_tables\n",
            "DB Call: execute_query\n",
            "The cheapest product is the usb charger at $25.00.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_ = chat.send_message('and how much it is?', request_options=retry_policy)\n",
        "print(r_.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Df5ghn03MGUa",
        "outputId": "602ca812-2c82-4eb7-f03d-bab3ff0d8035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cheapest product, the usb charger, costs $25.00.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}