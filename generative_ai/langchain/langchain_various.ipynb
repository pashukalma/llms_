{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Traditional Software application\n",
        "- Client --> frontend --> backend --> database\n",
        "\n",
        "### LLM app\n",
        "- Client --> Prompt --> LLM --> Parser output\n",
        "\n",
        "```\n",
        "- client layer to collect user input as text queries or decisions, - prompt engineering\n",
        "layer to construct and guide LLM, - LLM backend to analyze prompts and produce relevant\n",
        "test responses, - output parser to interpret LLM response for application, - integration\n",
        "with services via functional APIs, knowledge store and other sources and algorithms to\n",
        "augment LLM's capabilities\n",
        "LLM integrates services via - functional APIs, - advanced reasoning algroithms for complex\n",
        "logic chains, - retrieval augmented via knowledge base\n",
        "```"
      ],
      "metadata": {
        "id": "a2sqKcfkdIcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Benefits\n",
        "```\n",
        "- Modular architecture, flexible and adaptable LLM integrations, - Composability, chaining\n",
        "together multiple services, - Goal driven agents, plan chain of logic for specific goals\n",
        "  - dynamic responses, statefulness, robustness, composition\n",
        "- Memory and persistence for statefulness across executions, - Readability, Maintability,\n",
        "Reusability, Tool Integration, Productivity\n",
        "Standard interfaces for memory integration with data stores/DBs\n",
        "- ConversattionBufferMemory, ConversattionBufferWindowMemory, for all messages in history or\n",
        "recent messages, - ConversattionKGMemory for knowledge graph exchanges/prompt use,\n",
        "- ConversattionEntityMemory to store facts from conversation\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Nyv0Tg7zj_nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database options\n",
        "```\n",
        "- SQL Postgress and SQLite, NoSQL choices MongoDB/Cassandra, - Redis for in-memory high\n",
        "performance caching, - Managed cloud services like DynamoDB to remove infrastructure burden\n",
        "Tools for modular interfaces, there are many to mention a few - ML tools, wikipedia, Maps,\n",
        "Weather, Stocks, Search engine, knowledge graphs,\n",
        "```"
      ],
      "metadata": {
        "id": "XFMH5QtDnLcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain package structure\n",
        "```\n",
        "- langchain-core, main langchain, langchain-experimental, langchain-community,\n",
        "partner-packages (langchain-ai, langchain-anthropic, google-APIs,\n",
        "langchain google-cloud-sql-mssql package, ...)\n",
        "```"
      ],
      "metadata": {
        "id": "W_BKwurcjAi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "n3scoKSNSCm2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "St-4kFctR9qz"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain_openai langchain_experimental scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' https://platform.openai.com/docs/models for a list of models '''\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "def set_environment():\n",
        "  variable_dict = globals().items()\n",
        "  for key, value in variable_dict:\n",
        "    if 'API' in key or 'ID' in key:\n",
        "      os.environ[key] = value\n",
        "set_environment()"
      ],
      "metadata": {
        "id": "Ud_fQ6crVr_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "llm_ = OpenAI()\n",
        "llm_"
      ],
      "metadata": {
        "id": "YHOA3c9lSCuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa13ec4-ab3d-418b-8e7f-c328928e470e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAI(client=<openai.resources.completions.Completions object at 0x7a90c8b16a90>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7a90c8b2c7d0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "WP2_XYhMrF77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "client = genai.Client(api_key=os.environ['GOOGLE_API_KEY'])"
      ],
      "metadata": {
        "id": "EZvv3Z5ksgP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Integration '''\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "llm_hf = HuggingFaceHub(\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512},\n",
        "    repo_id=\"google/flan-t5-xxl\")\n",
        "llm_hf"
      ],
      "metadata": {
        "id": "sB92vIusSC6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "llm_chat_ = ChatOpenAI(model_name='gpt-4o-mini')\n",
        "response = llm_chat_.invoke([HumanMessage(\n",
        "    content='Welcome in Python')])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "AgarOn0hSDhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d594dd40-61dc-4871-e8a1-44752102ac94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Welcome! If you're looking to get started with Python or have any specific questions about it, feel free to ask! Whether you're interested in basic syntax, data structures, libraries, or specific projects, I'm here to help. What would you like to know or discuss?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 10, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-b2b3a282-9f74-4d0c-8cb9-b2a4622af9d3-0' usage_metadata={'input_tokens': 10, 'output_tokens': 54, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uZNEih5c2nN",
        "outputId": "79ea5fc6-84aa-4f4b-cbd5-7a0441c65521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"content\": \"Welcome! If you're interested in Python, there are many exciting topics we can explore together. Whether you're looking to learn the basics, dive into advanced concepts, or work on specific projects, I'm here to help. What would you like to know or discuss about Python?\", \"additional_kwargs\": {}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 54, \"prompt_tokens\": 10, \"total_tokens\": 64, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}, \"model_name\": \"gpt-4o-mini\", \"system_fingerprint\": \"fp_72ed7ab54c\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-f1f1f2d5-fdf2-49f6-8278-8301986de3bb-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.response_metadata)\n",
        "'''\n",
        "{'token_usage': {'completion_tokens': 54, 'prompt_tokens': 10, 'total_tokens': 64,\n",
        "'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0,\n",
        "'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details':\n",
        "{'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini',\n",
        "'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re3HFv5pc2rI",
        "outputId": "f8671943-4928-46cd-8ebb-7b4278934727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token_usage': {'completion_tokens': 54, 'prompt_tokens': 10, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_output = llm_chat_.invoke([\n",
        "    SystemMessage(content='You are a helpful assistant.'),\n",
        "    HumanMessage(content='Write a haiku about recursion in programming.')\n",
        "])"
      ],
      "metadata": {
        "id": "e1pWWrWmc20d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from IPython.display import Markdown, display, Image"
      ],
      "metadata": {
        "id": "f0GxxFlUjuuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()\n",
        "response = llm.invoke(prompt.format(text=\"Write a haiku about recursion in programming.\"))\n",
        "Markdown(response)"
      ],
      "metadata": {
        "id": "-Mk9iMr7Arw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_core\n",
        "from langchain_core.prompts import ProjectTemplate\n",
        "prompt_template = ProjectTemplate.from_template(\n",
        "    \"tell me a {adjective} joke about {content}.\")\n",
        "formatted_ = prompt_template.format(adjective=\"funny\", content=\"storytelling\")\n",
        "print(formatted_)"
      ],
      "metadata": {
        "id": "q0x1EjtciaI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_output = llm.invoke([\n",
        "    SystemMessage(content='You are a helpful assistant.'),\n",
        "    HumanMessage(content='What is the purpose of model regularization.')\n",
        "])\n",
        "Markdown(chat_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "uw_Ur-kmlRg8",
        "outputId": "7af29e5c-9f3e-429c-9761-a901f4ea8743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSystem: Model regularization is a technique used in machine learning and statistical models to prevent overfitting. Overfitting occurs when a model fits the training data very well, but performs poorly on unseen data. Regularization helps to reduce the complexity of the model and improve its generalization, making it more reliable for real-world applications. It does this by adding a penalty term to the model's loss function, which encourages simpler and more generalized solutions. This helps to prevent the model from memorizing the training data and instead learns patterns and relationships that can be applied to new data."
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LangChain Expression Language LCEL"
      ],
      "metadata": {
        "id": "AYqHQun5l2SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "''' define a chat-prompt-template to translate text '''\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'You are a helpful assistant that translates {input_language} to {output_language}.'),\n",
        "    ('user', 'translate this to French {text}')\n",
        "])\n",
        "text_ = \"What is the purpose of model regularization.\"\n",
        "llm_lcel = ChatOpenAI()\n",
        "response = llm_lcel.invoke(template.format_prompt(\n",
        "    input_language='English', output_language='French', text=text_))\n",
        "Markdown(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLJVRspXlRrK",
        "outputId": "3e026d30-1b78-4c39-9c73-5ec4c9f674ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Quel est le but de la régularisation du modèle."
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chains"
      ],
      "metadata": {
        "id": "NPfvK6YBoRqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm_gem = GoogleGenerativeAI(model = 'gemini-pro')\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer: Let's thing this step by step\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm_gem)\n",
        "question = \"query\"\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "8mXYsdkZoh51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Text-to-Image"
      ],
      "metadata": {
        "id": "5wosgC0YsJet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "Cow4Y3OUtt29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "  input_variables=['image_desc'],\n",
        "  template=(\n",
        "      'generate a concise prompt to generate am inmage with following desc'\n",
        "      \"{image_description}\"\n",
        "  )\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "image_url = DallEAPIWrapper().run(chain.run(\"night at San Francisco museum\"))\n",
        "display(Image(url=image_url))"
      ],
      "metadata": {
        "id": "0ISFXRP9oiEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get(image_url)\n",
        "image_path = 'generated_img'\n",
        "with open(image_path, 'wb') as f:\n",
        "  f.write(response.content)\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V45DgYF6sMcr",
        "outputId": "882a31a6-0c10-4f08-c196-cbf7753e05d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_img  requirements.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Replicate"
      ],
      "metadata": {
        "id": "SGrdNDLluzE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chat = ChatOpenAI(model = \"gpt-4-turbo\", max_tokens=256)\n",
        "llm_chat.invoke([\n",
        "    HumanMessage(\n",
        "      content=[\n",
        "        {\"type\": \"text\", \"text\": \"What is the image showing\"},\n",
        "        {\"type\": \"image_url\",\n",
        "         \"image_url\": {\"url\": image_url, \"detail\": \"auto\"}}\n",
        "    ])\n",
        "])"
      ],
      "metadata": {
        "id": "zGqEEI_IwRds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###HF Transformers"
      ],
      "metadata": {
        "id": "dlKMcffuyQOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generate_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"liminerity/Phigments12\", #Liph.42, MSL7/Liph.42-slerp\n",
        "    trust_remote_code = True,\n",
        "    torch_dtype = \"auto\",\n",
        "    device_map = \"auto\",\n",
        "    max_new_tokens=100)\n",
        "generate_text(\"Generative AI with Langchain in Python\")"
      ],
      "metadata": {
        "id": "c2Db1vGisMsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jAlF88kg5gQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "hugging_face = HuggingFacePipeline(pipeline=generate_text)"
      ],
      "metadata": {
        "id": "dBf78C6U133K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' langchain Prompt Template '''\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "template = \"\"\" {question} be concise \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=hugging_face)\n",
        "question = \"What is quantization in Machine Learning\"\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "apfTAZpv14FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' generate text from model with task text generation '''\n",
        "hugging_face = HuggingFacePipeline.from_model_id(\n",
        "    model_id = \"gpt2\", task = \"text-generation\",\n",
        "    pipeline_kwargs = {\"max_new_tokens\": 100}\n",
        ")\n",
        "llm_chain = prompt | hugging_face\n",
        "question = \"What is quantization in Machine Learning\"\n",
        "Markdown(llm_chain.invoke(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "OqLkAq2p14La",
        "outputId": "8f3589e4-0b5a-42fa-c160-400e1fcd14d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " What is quantization in Machine Learning be concise \"\n\nHe told TechCrunch that his previous projects are to create a new kind of computer that works not only with data, but also with AI. He added that there are new questions that will become available as the world moves through the Internet as well as technologies like machine learning and machine learning analytics.\n\nHe also mentioned that Google plans to make Google Earth accessible \"to everyone.\" He added that this will \"make a huge difference for the planet.\""
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GPT4All\n",
        "- GPT - Llama - Replit"
      ],
      "metadata": {
        "id": "LLKcXk3C5DFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' document classification '''"
      ],
      "metadata": {
        "id": "pgtnNE_w14QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' text summarization '''"
      ],
      "metadata": {
        "id": "YMTm7TOQ5Wj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' applying map-reduce '''"
      ],
      "metadata": {
        "id": "G2F3O0q05WpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Architecture"
      ],
      "metadata": {
        "id": "5nOjxRRh91on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan-and-solve research plan"
      ],
      "metadata": {
        "id": "YK1w1qLQCkWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import *\n",
        "from langchain import hub\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.chains.base import Chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.plan_and_execute import \\\n",
        "                    PlanAndExecute, load_agent_executor, load_chat_planner\n",
        "# import question answering tool\n",
        "\n",
        "ReasoningStrategies = Literal[\n",
        "    'zero-shot-react', 'plan-and-solve'\n",
        "]"
      ],
      "metadata": {
        "id": "C_OVcE3X5W5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "\n",
        "repl_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. \\\n",
        "    Use this to execute python commands. \\\n",
        "    Input should be a valid python command. \\\n",
        "    If you want to see the output of a value, \\\n",
        "    you should print it out with `print(...)`\",\n",
        "    func=python_repl.run\n",
        ")\n",
        "prompt = PromptTemplate(\n",
        "template = '''Answer the following questions the best you can\\\n",
        "with the following tools\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "Question: input question to answer\n",
        "Thought: Think about what to do\n",
        "Action: Action to take {tool_names}\n",
        "Observation: the result of the action\n",
        "... (this) Thought/Action/Action Input/ Observation repeat N times\n",
        "Thought: I now know the final answer\n",
        "Final Answer: The final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought: {agent_scratchpad}\n",
        "'''\n",
        ")\n",
        "llm = OpenAI()\n",
        "agent = create_react_agent(llm, [repl_tool], prompt)\n"
      ],
      "metadata": {
        "id": "rQDrb0gEvp41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(python_repl.run(\"print(1+1)\"))\n",
        "llm = ChatOpenAI(temperature=0, streaming = True)\n",
        "print(llm.invoke(\"what is 2+2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgQE4XtgwEyS",
        "outputId": "6b8be07b-75b7-4d96-901d-c8fba9800635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "\n",
            "content='2 + 2 equals 4.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-760e5089-a604-4e0a-90dc-721b383f31ce-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "rEb4QeMbz4IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import load_tools\n",
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "python_repl = PythonREPLTool()\n",
        "wikipedia_api_wrapper = WikipediaAPIWrapper(lang=\"en\", top_k_results=3)\n",
        "\n",
        "'''\n",
        "def load_tools\n",
        "  - google search,\n",
        "  - wikipedia,\n",
        "  - ddg search.\n",
        "  - prompt-search,\n",
        "  - python_repl,\n",
        "  - llm-math,\n",
        "  - critical-search\n",
        "'''\n"
      ],
      "metadata": {
        "id": "deocSblUyhRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_agent(tool_names: list[str], strategy: ReasoningStrategies = \"zero-shot-react\"):\n",
        "    llm = ChatOpenAI(temperature=0, streaming = True)\n",
        "    tools = load_tools(tool_names=tool_names, llm=llm)\n",
        "\n",
        "    '''\n",
        "    if strategy == \"zero-shot-react\":\n",
        "        agent = create_zero_shot_agent(llm=llm, tools=tools, verbose=True)\n",
        "    '''\n",
        "    if strategy == \"plan-and-solve\":\n",
        "        planner = load_chat_planner(llm)\n",
        "        executor = load_agent_executor(llm, tools, verbose=True)\n",
        "        return PlanAndExecute(planner=planner, executor=executor, verbose=True)\n",
        "    prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "    return AgentExecutor(agent=create_react_agent(\n",
        "        llm=llm, tools=tools, prompt=prompt, verbose=True\n",
        "    ), tools=tools)"
      ],
      "metadata": {
        "id": "lXyODCH7nEbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Ck9KwcS5E3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "strategy = st.radio('Reasoning Strategy', ('zero-shot-react', 'plan-and-solve'))\n",
        "tool_names = st.multiselect(\n",
        "    \"which tool to use?\",\n",
        "    [\n",
        "        'google-search', 'ddg-search', 'wolfram-alpha', 'wikipedia',\n",
        "        'pal-math', 'llm-math', 'arxiv'\n",
        "    ],\n",
        "    default=['google-search', 'ddg-search', 'wolfram-alpha', 'wikipedia',\n",
        "        'pal-math', 'llm-math', 'arxiv']\n",
        ")"
      ],
      "metadata": {
        "id": "qwnDfKbc5W9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain = load_agent(tool_names, strategy)"
      ],
      "metadata": {
        "id": "TStHPr_B5MHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Class GPT Researcher"
      ],
      "metadata": {
        "id": "i2Lb8eiXIbRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tbd"
      ],
      "metadata": {
        "id": "lYW4M0U95XCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building DeepResearch Tool"
      ],
      "metadata": {
        "id": "kCx7GI6LeN0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tbd"
      ],
      "metadata": {
        "id": "ux3WLNiteRVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Fact checking Pipeline"
      ],
      "metadata": {
        "id": "WibrgPefStkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here is a statement: {statement} \\n\n",
        "Make a bullet point list of the assumptions made in producing the statement\n",
        "\n",
        "Here is bullet point list of the assertions:\n",
        "{assertions}\n",
        "For each assertion, determine whether it is true or false. Please explain, why\n",
        "\n",
        "In light of the above facts, how would you answer the {question}\n",
        "'''"
      ],
      "metadata": {
        "id": "S-ttDh-osMvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.245"
      ],
      "metadata": {
        "id": "j1blVstbVGc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMCheckerChain\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4\", temperature=0.7)\n",
        "text = \"Which search engine is the most popular\"\n",
        "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
        "checker_chain.invoke(text)"
      ],
      "metadata": {
        "id": "c7tHauQYdNGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "text = \"today is Monday Februar 3rd\"\n",
        "query = embeddings.embed_query(text)\n",
        "print(query)"
      ],
      "metadata": {
        "id": "AkXGGBKYdQDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting structured information from documents"
      ],
      "metadata": {
        "id": "DA6W3pjlhPXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' resume example\n",
        "\n",
        "Design\n",
        "Classes: Base, specific, resume\n",
        "   Experience(start-date, end-date, description)\n",
        "   WorkExperience(company, job-title)\n",
        "   Education(degree, university)\n",
        "   Resume(\n",
        "    first-name, last-name, linkedin-url, email, skill- str,\n",
        "    study - Education,\n",
        "    work-experience - WorkExperience)\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter\n",
        "\n",
        "parse PDF\n",
        "parsers: PydanticOutputParser, SimpleOutputParser\n",
        "build the template to extract the information based on instructions\n",
        "use gpt-4-turbo and invoke the chain for the document(s)\n",
        "\n",
        "langchain outparsers:\n",
        "- Json, XML, Pydantic, yaml, Pandas DataFrame\n",
        "from AI message extract the Json or other format\n",
        "'''"
      ],
      "metadata": {
        "id": "YfdpAMBIjdm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building llm like GPT\n",
        "- from Vectors to RAG\n",
        "- Vectors Embeddings\n",
        "- Embeddings in LangChain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tCQI6YOpdpFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Embeddings models in OpenAI\n",
        "\n",
        "text-embedding-3-large - 3072 output dimenson\n",
        "text-embedding-3-small - 1536\n",
        "text-embedding-ada-002 - 1536\n",
        "'''"
      ],
      "metadata": {
        "id": "zzD4vAhMjd9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "OJP2a5fppKJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"Natural\", \"language\", \"processing\", \"Deep\", \"learning\"]\n",
        "vectors_emb = hf_embeddings.embed_documents(words)\n",
        "vectors_emb[:1][:1][0][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfPYfbS2pKNZ",
        "outputId": "0b35a80a-b256-4751-d3f8-75f9b0b2c411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.009212962351739407, -0.006104120519012213, -0.008195655420422554]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'operations with vectors - calculate distances'"
      ],
      "metadata": {
        "id": "A1kA5R_eriSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "import numpy as np, pandas as pd\n",
        "X = np.array(vectors_emb)\n",
        "distances = squareform(pdist(X))\n",
        "distances"
      ],
      "metadata": {
        "id": "m2YRU0IlrifI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(distances, columns=words, index=words)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j2bJenNHrijM",
        "outputId": "afd37bdd-bf99-438a-cb30-742059061256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Natural  language  processing      Deep  learning\n",
              "Natural     0.000000  0.523703    0.570454  0.508168  0.560684\n",
              "language    0.523703  0.000000    0.543969  0.550727  0.485309\n",
              "processing  0.570454  0.543969    0.000000  0.578775  0.541797\n",
              "Deep        0.508168  0.550727    0.578775  0.000000  0.543350\n",
              "learning    0.560684  0.485309    0.541797  0.543350  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1592774-f02a-4a44-8f41-f36a486b5f05\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Natural</th>\n",
              "      <th>language</th>\n",
              "      <th>processing</th>\n",
              "      <th>Deep</th>\n",
              "      <th>learning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Natural</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.523703</td>\n",
              "      <td>0.570454</td>\n",
              "      <td>0.508168</td>\n",
              "      <td>0.560684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>language</th>\n",
              "      <td>0.523703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.543969</td>\n",
              "      <td>0.550727</td>\n",
              "      <td>0.485309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>processing</th>\n",
              "      <td>0.570454</td>\n",
              "      <td>0.543969</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.578775</td>\n",
              "      <td>0.541797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep</th>\n",
              "      <td>0.508168</td>\n",
              "      <td>0.550727</td>\n",
              "      <td>0.578775</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.543350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.560684</td>\n",
              "      <td>0.485309</td>\n",
              "      <td>0.541797</td>\n",
              "      <td>0.543350</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1592774-f02a-4a44-8f41-f36a486b5f05')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1592774-f02a-4a44-8f41-f36a486b5f05 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1592774-f02a-4a44-8f41-f36a486b5f05');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9096edf7-dd01-4b58-97ee-154518331a35\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9096edf7-dd01-4b58-97ee-154518331a35')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9096edf7-dd01-4b58-97ee-154518331a35 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c4eb4159-c017-4c27-8b87-97f5fb3962fc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c4eb4159-c017-4c27-8b87-97f5fb3962fc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Natural\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2431883885073894,\n        \"min\": 0.0,\n        \"max\": 0.5704543593493874,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5237026712773398,\n          0.5606839346184048,\n          0.5704543593493874\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23657693439707092,\n        \"min\": 0.0,\n        \"max\": 0.5507270941627679,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.48530870259393893,\n          0.5439689019898217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processing\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.25040167456239154,\n        \"min\": 0.0,\n        \"max\": 0.5787750187696163,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5439689019898217,\n          0.5417965829166063,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Deep\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24514028279534375,\n        \"min\": 0.0,\n        \"max\": 0.5787750187696163,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5507270941627679,\n          0.5433496195545624,\n          0.5787750187696163\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23995458261995598,\n        \"min\": 0.0,\n        \"max\": 0.5606839346184048,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.48530870259393893,\n          0.0,\n          0.5417965829166063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.style.background_gradient(cmap=\"coolwarm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NNRp3yuerin8",
        "outputId": "a916ec61-5427-46e7-8e89-b613606c4190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7c0bc81c7110>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4b5a9_row0_col0, #T_4b5a9_row1_col1, #T_4b5a9_row2_col2, #T_4b5a9_row3_col3, #T_4b5a9_row4_col4 {\n",
              "  background-color: #3b4cc0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row0_col1, #T_4b5a9_row1_col3 {\n",
              "  background-color: #c53334;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row0_col2, #T_4b5a9_row2_col1 {\n",
              "  background-color: #b8122a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row0_col3 {\n",
              "  background-color: #dd5f4b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row0_col4, #T_4b5a9_row2_col0, #T_4b5a9_row2_col3, #T_4b5a9_row3_col1, #T_4b5a9_row3_col2 {\n",
              "  background-color: #b40426;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row1_col0 {\n",
              "  background-color: #d0473d;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row1_col2, #T_4b5a9_row4_col3 {\n",
              "  background-color: #ca3b37;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row1_col4 {\n",
              "  background-color: #e0654f;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row2_col4 {\n",
              "  background-color: #c0282f;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row3_col0 {\n",
              "  background-color: #d85646;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row3_col4 {\n",
              "  background-color: #be242e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row4_col0 {\n",
              "  background-color: #ba162b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row4_col1 {\n",
              "  background-color: #dc5d4a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4b5a9_row4_col2 {\n",
              "  background-color: #cb3e38;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4b5a9\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4b5a9_level0_col0\" class=\"col_heading level0 col0\" >Natural</th>\n",
              "      <th id=\"T_4b5a9_level0_col1\" class=\"col_heading level0 col1\" >language</th>\n",
              "      <th id=\"T_4b5a9_level0_col2\" class=\"col_heading level0 col2\" >processing</th>\n",
              "      <th id=\"T_4b5a9_level0_col3\" class=\"col_heading level0 col3\" >Deep</th>\n",
              "      <th id=\"T_4b5a9_level0_col4\" class=\"col_heading level0 col4\" >learning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4b5a9_level0_row0\" class=\"row_heading level0 row0\" >Natural</th>\n",
              "      <td id=\"T_4b5a9_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
              "      <td id=\"T_4b5a9_row0_col1\" class=\"data row0 col1\" >0.523703</td>\n",
              "      <td id=\"T_4b5a9_row0_col2\" class=\"data row0 col2\" >0.570454</td>\n",
              "      <td id=\"T_4b5a9_row0_col3\" class=\"data row0 col3\" >0.508168</td>\n",
              "      <td id=\"T_4b5a9_row0_col4\" class=\"data row0 col4\" >0.560684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b5a9_level0_row1\" class=\"row_heading level0 row1\" >language</th>\n",
              "      <td id=\"T_4b5a9_row1_col0\" class=\"data row1 col0\" >0.523703</td>\n",
              "      <td id=\"T_4b5a9_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "      <td id=\"T_4b5a9_row1_col2\" class=\"data row1 col2\" >0.543969</td>\n",
              "      <td id=\"T_4b5a9_row1_col3\" class=\"data row1 col3\" >0.550727</td>\n",
              "      <td id=\"T_4b5a9_row1_col4\" class=\"data row1 col4\" >0.485309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b5a9_level0_row2\" class=\"row_heading level0 row2\" >processing</th>\n",
              "      <td id=\"T_4b5a9_row2_col0\" class=\"data row2 col0\" >0.570454</td>\n",
              "      <td id=\"T_4b5a9_row2_col1\" class=\"data row2 col1\" >0.543969</td>\n",
              "      <td id=\"T_4b5a9_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "      <td id=\"T_4b5a9_row2_col3\" class=\"data row2 col3\" >0.578775</td>\n",
              "      <td id=\"T_4b5a9_row2_col4\" class=\"data row2 col4\" >0.541797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b5a9_level0_row3\" class=\"row_heading level0 row3\" >Deep</th>\n",
              "      <td id=\"T_4b5a9_row3_col0\" class=\"data row3 col0\" >0.508168</td>\n",
              "      <td id=\"T_4b5a9_row3_col1\" class=\"data row3 col1\" >0.550727</td>\n",
              "      <td id=\"T_4b5a9_row3_col2\" class=\"data row3 col2\" >0.578775</td>\n",
              "      <td id=\"T_4b5a9_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
              "      <td id=\"T_4b5a9_row3_col4\" class=\"data row3 col4\" >0.543350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b5a9_level0_row4\" class=\"row_heading level0 row4\" >learning</th>\n",
              "      <td id=\"T_4b5a9_row4_col0\" class=\"data row4 col0\" >0.560684</td>\n",
              "      <td id=\"T_4b5a9_row4_col1\" class=\"data row4 col1\" >0.485309</td>\n",
              "      <td id=\"T_4b5a9_row4_col2\" class=\"data row4 col2\" >0.541797</td>\n",
              "      <td id=\"T_4b5a9_row4_col3\" class=\"data row4 col3\" >0.543350</td>\n",
              "      <td id=\"T_4b5a9_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vector Storage\n",
        "- indexing - algorithm approximate nearest neighbors\n",
        "- vector libraries for operations\n",
        "- vector databases for storage, retrieval, and manage set of vectors (like Pinecone)\n"
      ],
      "metadata": {
        "id": "_5xdZdONtMjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Algorithms/Techniques\n",
        "- Product quantization  - Locality sensitive hashing LSH\n",
        "- Hierarchical navigable small world (HNSW)\n",
        "- HNSW, KNN, Graph Neural Networks GNNs, Graph Convolutional Networks GCNs\n",
        "\n",
        "###Vector libraries\n",
        "- ANN Approximate nearest neighbor - Faiss Facebook AI Similarity Search\n",
        "- annoy C++ for high dimensional data (genomics) - hnswlib memory efficient indexing\n",
        "- nmslib non metric space open source for similarity search and,\n",
        "- SPTAG Microsoft's ANN  (k-d tree for neighborhood graph)"
      ],
      "metadata": {
        "id": "p6uDJEZMwMgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vector Databases\n",
        "- Anomaly detection (fraud detection, network security, system monitoring)\n",
        "- Personalization - NLP\n",
        "\n",
        "- Efficient retrieval of similar vectors - Specialized for specific tasks\n",
        "- Support for high dimensional spaces - Enables advanced search capabilities\n",
        "\n",
        "Vendors: Chroms, Qdrant, Milvus, Pinecone, ..."
      ],
      "metadata": {
        "id": "2F4hsay2vYd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Loading and retrieving with LangChain\n",
        "\n",
        "Data Source --> Load --> Transform --> Embed --> Store --> Retrieve\n",
        "Connection                             vec_emb   VecDB    transformed-format\n",
        "'''"
      ],
      "metadata": {
        "id": "dhxeBzoHtFLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' use Chroms Vector Store in LangChain  1. Load\n",
        "\n",
        "Chroma is the optimized backend for storing and retrieving vector documents\n",
        "- load book reviews from amazon\n",
        "'''\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "url_ = \"https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618\"\n",
        "loader = WebBaseLoader(url_)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYRQPGxRvW1X",
        "outputId": "9fe5d8f9-c136-46f1-a42b-c07604f66102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' split the document into chunks - can be optional 2. Transform'''\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=0,\n",
        "    separator=\"\\nReport\")\n",
        "split_documents = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "33S9JvmKtFxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Import Chroms and Embedding modules 3. Embedding 4. VectorDB store '''\n",
        "from langchain_community.vectorstores import Chroma\n",
        "#from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=split_documents,\n",
        "    embedding=CohereEmbeddings(model=\"embed-english-v3.0\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FsFD7ebrjeCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Retrieve from VectorDB 5. Retrieval similarity vectors'''\n",
        "#similar_vectors = vectorstore.similarity_search(\"Deep Learning\", k=2)\n",
        "similar_vectors = vectorstore.similarity_search(\n",
        "    \"This is a fantastic book\", k=1)\n",
        "print(similar_vectors)"
      ],
      "metadata": {
        "id": "HFpD2jh80ZXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Retrievers: KNN and PubMed retriever"
      ],
      "metadata": {
        "id": "0ArQCDh47Nr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import KNNRetriever\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "words= ['Natural', 'language', 'processing', 'Deep', 'learning']\n",
        "retriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())\n",
        "result = retriever.get_relevant_documents(\"Deep Learning\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsGn9Yjm45cv",
        "outputId": "7fbeece1-4616-430d-e130-abe8da2c6da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Deep'),\n",
              " Document(metadata={}, page_content='learning'),\n",
              " Document(metadata={}, page_content='processing'),\n",
              " Document(metadata={}, page_content='language')]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.k = 2\n",
        "result = retriever.get_relevant_documents(\"Deep Learning\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIi06ob945kP",
        "outputId": "ea2410cc-2e72-4b42-96af-c3e5f8513244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Deep'),\n",
              " Document(metadata={}, page_content='learning')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' PubMMed Retriever '''\n",
        "from langchain_community.retrievers.pubmed import PubMedRetriever\n",
        "retriever = PubMedRetriever()\n",
        "documents = retriever.get_relevant_documents(\"COVID\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbvnhv7b45rt",
        "outputId": "59984700-ceff-43c7-c8d1-eed10ff31c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Too Many Requests, waiting for 0.20 seconds...\n",
            "Too Many Requests, waiting for 0.40 seconds...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "  print(document.metadata[\"Title\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK4NYZzx45u4",
        "outputId": "e8ec096d-bc0b-4eb5-fc18-32a624cc8b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Induction of the Inflammasome by the SARS-CoV-2 Accessory Protein ORF9b, Abrogated by Small-Molecule ORF9b Homodimerization Inhibitors.\n",
            "The associations between arts and humanities engagement and well-being in a representative sample of United States residents during the COVID-19 pandemic.\n",
            "An updated review of pulmonary radiological features of acute and chronic pulmonary COVID-19.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom retriever"
      ],
      "metadata": {
        "id": "vqgmGhwq-z5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 1-2. Document loader and transform step '''\n",
        "from typing import *\n",
        "import logging, pathlib, os, tempfile\n",
        "from langchain_community.document_loaders import  PyPDFLoader, TextLoader, \\\n",
        "                      UnstructuredWordDocumentLoader, UnstructuredEPubLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class EPubReader(UnstructuredEPubLoader):\n",
        "  def __init__(self, file_path: str | list[str], **kwargs: Any):\n",
        "    super().__init__(file_path, **kwargs, mode=\"elements\", strategy=\"fast\")\n",
        "\n",
        "class DocumentLoaderException(Exception):\n",
        "  pass\n",
        "\n",
        "class DocumentLoader(object):\n",
        "  supported_extensions = {\n",
        "      \".pdf\": PyPDFLoader,\n",
        "      \".txt\": TextLoader,\n",
        "      \".docx\": UnstructuredWordDocumentLoader,\n",
        "      \".doc\": UnstructuredWordDocumentLoader,\n",
        "      \".epub\": EPubReader\n",
        "  }\n",
        "\n",
        "def load_document(file_path):\n",
        "  ''' load a file and return a list of documents '''\n",
        "  ext = pathlib.Path(file_path).suffix\n",
        "  '''\n",
        "  if ext not in DocumentLoader.supported_extensions:\n",
        "    raise DocumentLoaderException(f\"Unsupported file extension: {ext}\")\n",
        "  loader_class = DocumentLoader.supported_extensions[ext]\n",
        "  '''\n",
        "  loader = DocumentLoader.supported_extensions.get(ext)\n",
        "  if not loader:\n",
        "    raise DocumentLoaderException(f\"Unsupported file extension: {ext}\")\n",
        "  loader = loader(file_path)\n",
        "  documents = loader.load()\n",
        "  logging.info(documents)\n",
        "  return documents"
      ],
      "metadata": {
        "id": "7XCK8x7r-u2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 3. Vector storage '''\n",
        "from langchain_community.vectorstores.docarray import DocArrayInMemorySearch\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "\n",
        "def config_retriever(documents):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=1500, chunk_overlap=200) #, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "  splits = text_splitter.split_documents(documents)\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "  vectorstoreDB = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
        "  ''' mmr - max marginal relevance '''\n",
        "  retriever = vectorstoreDB.as_retriever(\n",
        "      search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4 } )\n",
        "  return retriever"
      ],
      "metadata": {
        "id": "IxAB136M-u-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 4. chat with retriever '''\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "def configure_chain(retriever: BaseRetriever):\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
        "  Q_and_A_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm, retriever=retriever, memory=memory, max_tokens_limit=4000, verbose=True)\n",
        "  return Q_and_A_chain\n",
        "\n",
        "def configure_Q_and_A_chain(uploaded_documents):\n",
        "  documents = []\n",
        "  tmp_dir = tempfile.TemporaryDirectory()\n",
        "  for file in uploaded_files:\n",
        "    file_path = os.path.join(tmp_dir.name, file.name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "      f.write(file.getvalue())\n",
        "    documents.extend(load_document(file_path))\n",
        "\n",
        "  retriever__ = config_retriever(documents)\n",
        "  Q_and_A_chain = configure_chain(retriever=retriever__)\n",
        "  return Q_and_A_chain"
      ],
      "metadata": {
        "id": "NP_rbKRV9pUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 5. Interface with logic '''\n",
        "import streamlit as st\n",
        "from streamlit.external.langchain import StreamlitCallbackHandler"
      ],
      "metadata": {
        "id": "TXRpdSn69psY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.set_page_config(page_title=\"LangChain: Chat with Documents\", page_icon=\"🦜\")\n",
        "st.title(\"🦜 LangChain: Chat with Documents \")\n",
        "uploaded_files = st.file_uploader(label=\"Upload files\",\n",
        "  type=list(DocumentLoader.supported_extensions.keys()), accept_multiple_files=True,)\n",
        "\n",
        "if not uploaded_files:\n",
        "  st.info(\"Please upload a file\")\n",
        "  st.stop()\n",
        "\n",
        "Q_and_A_chain = configure_Q_and_A_chain(uploaded_files)\n",
        "chat_assistant = st.chat_message(\"assistant like ChatGpt\")\n",
        "user_query = st.chat_input(\"Ask questions about the documents\")\n",
        "if user_query:\n",
        "  streamhandler_ = StreamlitCallbackHandler(chat_assistant)\n",
        "  '''\n",
        "  with st.chat_message(\"user\"):\n",
        "    st.write(user_query)\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    message_placeholder = st.empty()\n",
        "  '''\n",
        "  response = Q_and_A_chain.run(user_query, callbacks=[streamhandler_])\n",
        "  st.write(response)"
      ],
      "metadata": {
        "id": "N20eFEsLHPgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=.streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y09pUgwmK4yd",
        "outputId": "9b972060-da4c-44c2-b242-464b2382d1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.102.4:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}